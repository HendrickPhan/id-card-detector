{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5.git # clone\n",
    "%cd yolov5\n",
    "!pip install --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "%pip install -r requirements.txt # install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-2.1.0.dev20230313-cp39-none-macosx_11_0_arm64.whl (56.3 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56.3 MB 967 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/nightly/cpu/torchvision-0.15.0.dev20230313-cp39-cp39-macosx_11_0_arm64.whl (1.5 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.5 MB 249 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages (from torch) (4.4.0)\n",
      "Collecting filelock\n",
      "  Downloading https://download.pytorch.org/whl/nightly/filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting sympy\n",
      "  Using cached https://download.pytorch.org/whl/nightly/sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Requirement already satisfied: networkx in /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: numpy in /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: requests in /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages (from requests->torchvision) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, filelock, torch, torchvision\n",
      "Successfully installed filelock-3.9.0 mpmath-1.3.0 sympy-1.11.1 torch-2.1.0.dev20230313 torchvision-0.15.0.dev20230313\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v7.0-120-g3e55763 Python-3.9.6 torch-2.1.0.dev20230313 CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete ‚úÖ (10 CPUs, 16.0 GB RAM, 315.7/460.4 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import utils\n",
    "display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.downloads import attempt_download\n",
    "\n",
    "# p5 = ['n', 's', 'm', 'l', 'x']  # P5 models\n",
    "p5 = ['n']  # P5 models\n",
    "cls = [f'{x}-seg' for x in p5]  # segmentation models\n",
    "\n",
    "for x in cls:\n",
    "    attempt_download(f'weights/yolov5{x}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1msegment/train: \u001b[0mweights=yolov5n-seg.pt, cfg=, data=data/id-cards/data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=5, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train-seg, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, mask_ratio=4, no_overlap=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
      "YOLOv5 üöÄ v7.0-120-g3e55763 Python-3.9.6 torch-2.1.0.dev20230313 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-seg', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=8\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
      "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
      "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
      "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n",
      "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
      "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
      " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
      " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
      " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 24      [17, 20, 23]  1    136981  models.yolo.Segment                     [8, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 32, 64, [64, 128, 256]]\n",
      "Model summary: 225 layers, 1894133 parameters, 1894133 gradients, 6.9 GFLOPs\n",
      "\n",
      "Transferred 361/367 items from yolov5n-seg.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 60 weight(decay=0.0), 63 weight(decay=0.0005), 63 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/yolov5/data/id-cards/train/images/001087001290-1.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/yolov5/data/id-cards/train/images/001087001290-2.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [00:00<00:00, 1184.2\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/yo\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/yolov5/data/id-cards/train/images/001087001290-1.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/yolov5/data/id-cards/train/images/001087001290-2.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB ram): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [00:00<00:00, 392.30it\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.33 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Plotting labels to runs/train-seg/exp4/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train-seg/exp4\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/4         0G     0.1238     0.1253    0.04533    0.06806         57   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 1.250s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        271        430    0.00163     0.0285   0.000875   0.000295    0.00116      0.024   0.000631   0.000239\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/4         0G    0.09091    0.03969     0.0353    0.05657         32   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mWARNING ‚ö†Ô∏è NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        271        430    0.00574      0.492     0.0105    0.00378    0.00787      0.341     0.0104    0.00329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/4         0G    0.07035    0.02818    0.03694    0.04702         39   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        271        430     0.0999       0.33     0.0796     0.0275     0.0935      0.336     0.0734     0.0193\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/4         0G    0.06249    0.02699    0.03335    0.04415         53   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        271        430      0.539       0.43      0.313      0.126      0.531      0.419      0.303      0.106\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/4         0G     0.0541    0.02423    0.03307    0.04127         53   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        271        430      0.502      0.404      0.382      0.163      0.501      0.404       0.38      0.145\n",
      "\n",
      "5 epochs completed in 0.229 hours.\n",
      "Optimizer stripped from runs/train-seg/exp4/weights/last.pt, 4.1MB\n",
      "Optimizer stripped from runs/train-seg/exp4/weights/best.pt, 4.1MB\n",
      "\n",
      "Validating runs/train-seg/exp4/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 165 layers, 1889221 parameters, 0 gradients, 6.8 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        271        430      0.506      0.404      0.384      0.163      0.502      0.404      0.381      0.146\n",
      "                 front        271         67      0.318      0.567      0.379      0.182      0.303      0.552      0.352      0.132\n",
      "                  back        271         62       0.31      0.677      0.422        0.2      0.309      0.677        0.4      0.114\n",
      "                avatar        271         67      0.362      0.582      0.469      0.163      0.355      0.582      0.469      0.168\n",
      "          finger print        271         62          1          0       0.36      0.209          1          0       0.36      0.204\n",
      "            front icon        271         67      0.307      0.358      0.329     0.0991      0.298      0.358      0.329     0.0929\n",
      "             back icon        271         62      0.244      0.645      0.414       0.17      0.246      0.661      0.426      0.173\n",
      "                 stamp        271         43          1          0      0.313      0.119          1          0      0.334      0.134\n",
      "Results saved to \u001b[1mruns/train-seg/exp4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python segment/train.py --img 640 --batch 16 --epochs 5 --data data/id-cards/data.yaml --weights yolov5n-seg.pt --cache"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert model to TFlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['./runs/train-seg/exp6/weights/best.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['tflite']\n",
      "YOLOv5 üöÄ v7.0-120-g3e55763 Python-3.9.6 torch-1.13.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 165 layers, 1889221 parameters, 0 gradients, 6.8 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs/train-seg/exp6/weights/best.pt with output shape (1, 25200, 45) (3.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.11.0...\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "2023-03-13 16:15:23.653101: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-13 16:15:23.653444: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
      "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
      "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
      "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  4                -1  1     29184  models.common.C3                        [64, 64, 2]                   \n",
      "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  6                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
      "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
      " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
      " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
      " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 24      [17, 20, 23]  1    136981  models.yolo.Segment                     [8, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 32, 64, [64, 128, 256], [640, 640]]\n",
      "WARNING:tensorflow:From /Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/env/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(1, 640, 640, 3)]   0           []                               \n",
      "                                                                                                  \n",
      " tf_conv (TFConv)               (1, 320, 320, 16)    1744        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf_conv_1 (TFConv)             (1, 160, 160, 32)    4640        ['tf_conv[0][0]']                \n",
      "                                                                                                  \n",
      " tfc3 (TFC3)                    (1, 160, 160, 32)    4704        ['tf_conv_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_7 (TFConv)             (1, 80, 80, 64)      18496       ['tfc3[0][0]']                   \n",
      "                                                                                                  \n",
      " tfc3_1 (TFC3)                  (1, 80, 80, 64)      28928       ['tf_conv_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_15 (TFConv)            (1, 40, 40, 128)     73856       ['tfc3_1[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_2 (TFC3)                  (1, 40, 40, 128)     156288      ['tf_conv_15[0][0]']             \n",
      "                                                                                                  \n",
      " tf_conv_25 (TFConv)            (1, 20, 20, 256)     295168      ['tfc3_2[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_3 (TFC3)                  (1, 20, 20, 256)     295680      ['tf_conv_25[0][0]']             \n",
      "                                                                                                  \n",
      " tfsppf (TFSPPF)                (1, 20, 20, 256)     164224      ['tfc3_3[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_conv_33 (TFConv)            (1, 20, 20, 128)     32896       ['tfsppf[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_upsample (TFUpsample)       (1, 40, 40, 128)     0           ['tf_conv_33[0][0]']             \n",
      "                                                                                                  \n",
      " tf_concat (TFConcat)           (1, 40, 40, 256)     0           ['tf_upsample[0][0]',            \n",
      "                                                                  'tfc3_2[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_4 (TFC3)                  (1, 40, 40, 128)     90496       ['tf_concat[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_39 (TFConv)            (1, 40, 40, 64)      8256        ['tfc3_4[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_upsample_1 (TFUpsample)     (1, 80, 80, 64)      0           ['tf_conv_39[0][0]']             \n",
      "                                                                                                  \n",
      " tf_concat_1 (TFConcat)         (1, 80, 80, 128)     0           ['tf_upsample_1[0][0]',          \n",
      "                                                                  'tfc3_1[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_5 (TFC3)                  (1, 80, 80, 64)      22720       ['tf_concat_1[0][0]']            \n",
      "                                                                                                  \n",
      " tf_conv_45 (TFConv)            (1, 40, 40, 64)      36928       ['tfc3_5[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_concat_2 (TFConcat)         (1, 40, 40, 128)     0           ['tf_conv_45[0][0]',             \n",
      "                                                                  'tf_conv_39[0][0]']             \n",
      "                                                                                                  \n",
      " tfc3_6 (TFC3)                  (1, 40, 40, 128)     74112       ['tf_concat_2[0][0]']            \n",
      "                                                                                                  \n",
      " tf_conv_51 (TFConv)            (1, 20, 20, 128)     147584      ['tfc3_6[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_concat_3 (TFConcat)         (1, 20, 20, 256)     0           ['tf_conv_51[0][0]',             \n",
      "                                                                  'tf_conv_33[0][0]']             \n",
      "                                                                                                  \n",
      " tfc3_7 (TFC3)                  (1, 20, 20, 256)     295680      ['tf_concat_3[0][0]']            \n",
      "                                                                                                  \n",
      " tf_segment (TFSegment)         ((1, 25200, 45),     136821      ['tfc3_5[0][0]',                 \n",
      "                                 (1, 32, 160, 160))               'tfc3_6[0][0]',                 \n",
      "                                                                  'tfc3_7[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,889,221\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,889,221\n",
      "__________________________________________________________________________________________________\n",
      "2023-03-13 16:15:27.746081: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2023-03-13 16:15:27.747319: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-03-13 16:15:27.748142: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-13 16:15:27.748154: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-03-13 16:15:27.748941: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-13 16:15:28.850773: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success ‚úÖ 10.2s, saved as runs/train-seg/exp6/weights/best_saved_model (7.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.11.0...\n",
      "WARNING:absl:Found untraced functions such as tf_conv_2_layer_call_fn, tf_conv_2_layer_call_and_return_conditional_losses, tf_conv_3_layer_call_fn, tf_conv_3_layer_call_and_return_conditional_losses, tf_conv_4_layer_call_fn while saving (showing 5 of 287). These functions will not be directly callable after loading.\n",
      "2023-03-13 16:16:32.727263: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-03-13 16:16:32.727292: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-03-13 16:16:32.728095: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/5x/z9p78txn1sn7j7qs1q4xzd2m0000gn/T/tmpi27f6hjk\n",
      "2023-03-13 16:16:32.752245: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-03-13 16:16:32.752268: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/5x/z9p78txn1sn7j7qs1q4xzd2m0000gn/T/tmpi27f6hjk\n",
      "2023-03-13 16:16:32.842199: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-03-13 16:16:32.863896: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-03-13 16:16:33.089201: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /var/folders/5x/z9p78txn1sn7j7qs1q4xzd2m0000gn/T/tmpi27f6hjk\n",
      "2023-03-13 16:16:33.193964: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 465872 microseconds.\n",
      "2023-03-13 16:16:33.543545: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-13 16:16:33.870622: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2111] Estimated count of arithmetic ops: 7.758 G  ops, equivalently 3.879 G  MACs\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success ‚úÖ 63.7s, saved as runs/train-seg/exp6/weights/best-fp16.tflite (3.7 MB)\n",
      "\n",
      "Export complete (74.1s)\n",
      "Results saved to \u001b[1m/Users/hieuphandinhminh/Desktop/Repo/projects/id-card-detection/yolov5/runs/train-seg/exp6/weights\u001b[0m\n",
      "Detect:          python segment/predict.py --weights runs/train-seg/exp6/weights/best-fp16.tflite \n",
      "Validate:        python segment/val.py --weights runs/train-seg/exp6/weights/best-fp16.tflite \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-seg/exp6/weights/best-fp16.tflite')  # WARNING ‚ö†Ô∏è SegmentationModel not yet supported for PyTorch Hub AutoShape inference\n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "!python export.py --weights ./runs/train-seg/exp6/weights/best.pt --include tflite"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
